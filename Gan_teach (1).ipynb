{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gan_teach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, ReLU, LeakyReLU, Dense\n",
        "from keras.layers.core import Activation, Reshape\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.core import Flatten\n",
        "#from keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from keras.datasets import cifar10\n",
        "from keras import initializers\n",
        "\n",
        "print(\"Libraries imported\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0jjwNClOjqw",
        "outputId": "9ebdcea2-de14-41e7-e2c1-1f9efc8ec150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    # load cifar10 data\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    # convert train and test data to float32\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # scale train and test data to [-1, 1]\n",
        "    X_train = (X_train / 255) * 2 - 1\n",
        "    X_test = (X_train / 255) * 2 - 1\n",
        "\n",
        "    return X_train, X_test\n",
        "    print(\"cifar10 data loaded\")"
      ],
      "metadata": {
        "id": "A0E7vKa4T6Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m8sA13DnUGJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cifar10_discriminator(ndf=64, image_shape=(32, 32, 3)):\n",
        "    \"\"\" Builds CIFAR10 DCGAN Discriminator Model\n",
        "    PARAMS\n",
        "    ------\n",
        "    ndf: number of discriminator filters\n",
        "    image_shape: 32x32x3\n",
        "\n",
        "    RETURN\n",
        "    ------\n",
        "    D: keras sequential\n",
        "    \"\"\"\n",
        "    init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "    D = Sequential()\n",
        "\n",
        "    # Conv 1: 16x16x64\n",
        "    D.add(Conv2D(ndf, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init,\n",
        "                 input_shape=image_shape))\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 2: 8x8x128\n",
        "    D.add(Conv2D(ndf*2, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 3: 4x4x256\n",
        "    D.add(Conv2D(ndf*4, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 4:  2x2x512\n",
        "    D.add(Conv2D(ndf*8, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Flatten: 2x2x512 -> (2048)\n",
        "    D.add(Flatten())\n",
        "\n",
        "    # Dense Layer\n",
        "    D.add(Dense(1, kernel_initializer=init))\n",
        "    D.add(Activation('sigmoid'))\n",
        "\n",
        "    print(\"\\nDiscriminator\")\n",
        "    D.summary()\n",
        "\n",
        "    return D"
      ],
      "metadata": {
        "id": "GDHfLmILPH0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RStkB4pBUZAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cifar10_generator(ngf=64, z_dim=128):\n",
        "    \"\"\" Builds CIFAR10 DCGAN Generator Model\n",
        "    PARAMS\n",
        "    ------\n",
        "    ngf: number of generator filters\n",
        "    z_dim: number of dimensions in latent vector\n",
        "\n",
        "    RETURN\n",
        "    ------\n",
        "    G: keras sequential\n",
        "    \"\"\"\n",
        "    init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "    G = Sequential()\n",
        "\n",
        "    # Dense 1: 2x2x512\n",
        "    G.add(Dense(2*2*ngf*8, input_shape=(z_dim, ),\n",
        "        use_bias=True, kernel_initializer=init))\n",
        "    G.add(Reshape((2, 2, ngf*8)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 1: 4x4x256\n",
        "    G.add(Conv2DTranspose(ngf*4, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 2: 8x8x128\n",
        "    G.add(Conv2DTranspose(ngf*2, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 3: 16x16x64\n",
        "    G.add(Conv2DTranspose(ngf, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 4: 32x32x3\n",
        "    G.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(Activation('tanh'))\n",
        "\n",
        "    print(\"\\nGenerator\")\n",
        "    G.summary()\n",
        "\n",
        "    return G"
      ],
      "metadata": {
        "id": "zMhKbx-fPPgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images, filename):\n",
        "    h, w, c = images.shape[1:]\n",
        "    grid_size = ceil(np.sqrt(images.shape[0]))\n",
        "    images = (images.reshape(grid_size, grid_size, h, w, c)\n",
        "              .transpose(0, 2, 1, 3, 4)\n",
        "              .reshape(grid_size*h, grid_size*w, c))\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.imsave(filename, images)"
      ],
      "metadata": {
        "id": "NDZvShxbPidh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(losses_d, losses_g, filename):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "    axes[0].plot(losses_d)\n",
        "    axes[1].plot(losses_g)\n",
        "    axes[0].set_title(\"losses_d\")\n",
        "    axes[1].set_title(\"losses_g\")\n",
        "    plt.tight_layout()#\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "VWMSUdJ-Pj1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize a list of sample image data in the range of 0 to 1\n",
        "    : x: List of image data.  The image shape is (32, 32, 3)\n",
        "    : return: Numpy array of normalized data\n",
        "    \"\"\"\n",
        "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))"
      ],
      "metadata": {
        "id": "8efaMdhWVlwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(ndf=64, ngf=64, z_dim=100, lr_d=2e-4, lr_g=2e-4, epochs=100,\n",
        "          batch_size=128, epoch_per_checkpoint=1, n_checkpoint_images=36):\n",
        "\n",
        "    X_train, _ = get_data()\n",
        "    image_shape = X_train[0].shape\n",
        "    print(\"image shape {}, min val {}, max val {}\".format(\n",
        "        image_shape, X_train[0].min(), X_train[0].max()))\n",
        "    X_train = normalize(X_train)\n",
        "\n",
        "    # plot real images for reference\n",
        "    plot_images(X_train[:n_checkpoint_images], \"real_images.png\")\n",
        "    \n",
        "\n",
        "    # build models\n",
        "    D = build_cifar10_discriminator(ndf, image_shape)\n",
        "    G = build_cifar10_generator(ngf, z_dim)\n",
        "\n",
        "    # define Discriminator's optimizer\n",
        "    D.compile(Adam(learning_rate=lr_d, beta_1=0.5), loss='binary_crossentropy',\n",
        "              metrics=['binary_accuracy'])\n",
        "\n",
        "    # define D(G(z)) graph for training the Generator\n",
        "    D.trainable = False\n",
        "    z = Input(shape=(z_dim, ))\n",
        "    D_of_G = Model(inputs=z, outputs=D(G(z)))\n",
        "\n",
        "    # define Generator's Optimizer\n",
        "    D_of_G.compile(Adam(learning_rate=lr_g, beta_1=0.5), loss='binary_crossentropy',\n",
        "                   metrics=['binary_accuracy'])\n",
        "\n",
        "    # get labels for computing the losses\n",
        "    labels_real = np.ones(shape=(batch_size, 1))\n",
        "    labels_fake = np.zeros(shape=(batch_size, 1))\n",
        "\n",
        "    losses_d, losses_g = [], []\n",
        "\n",
        "    # fix a z vector for training evaluation\n",
        "    z_fixed = np.random.uniform(-1, 1, size=(n_checkpoint_images, z_dim))\n",
        "\n",
        "    # training loop\n",
        "    for e in range(epochs):\n",
        "        print(\"Epoch {10}\".format(e))\n",
        "        for i in range(len(X_train) // batch_size):\n",
        "\n",
        "            # update Discriminator weights\n",
        "            D.trainable = True\n",
        "\n",
        "            # Get real samples\n",
        "            real_images = X_train[i*batch_size:(i+1)*batch_size]\n",
        "            loss_d_real = D.train_on_batch(x=real_images, y=labels_real)[0]\n",
        "\n",
        "            # Fake Samples\n",
        "            z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
        "            fake_images = G.predict_on_batch(z)\n",
        "            loss_d_fake = D.train_on_batch(x=fake_images, y=labels_fake)[0]\n",
        "\n",
        "            # Compute Discriminator's loss\n",
        "            loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
        "\n",
        "            # update Generator weights, do not update Discriminator weights\n",
        "            D.trainable = False\n",
        "            loss_g = D_of_G.train_on_batch(x=z, y=labels_real)[0]\n",
        "\n",
        "        losses_d.append(loss_d)\n",
        "        \n",
        "        losses_g.append(loss_g)\n",
        "        losses_d = normalize(losses_d)\n",
        "        losses_g = normalize(losses_g)\n",
        "\n",
        "\n",
        "        if (e % epoch_per_checkpoint) == 0:\n",
        "            print(\"loss_d={:.5f}, loss_g={:.5f}\".format(loss_d, loss_g))\n",
        "            fake_images = G.predict(z_fixed)\n",
        "            print(\"\\tPlotting images and losses\")\n",
        "            plot_images(fake_images, \"fake_images_e{}.png\".format(e))\n",
        "            plot_losses(losses_d, losses_g, \"losses.png\")\n",
        "\n",
        "train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B4xwPo4fPl3_",
        "outputId": "77debed1-73b2-439a-a7ec-a72a7316f83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n",
            "image shape (32, 32, 3), min val -1.0, max val 1.0\n",
            "\n",
            "Discriminator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 16, 16, 64)        4864      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 128)         204928    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8, 8, 128)        512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 256)         819456    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 2, 2, 512)         3277312   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 2, 2, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 2049      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,312,193\n",
            "Trainable params: 4,310,401\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "\n",
            "Generator\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 2048)              206848    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 2, 2, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 4, 4, 256)        3277056   \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 4, 4, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 128)        819328    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 64)       204864    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16, 16, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 32, 32, 3)        4803      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 3)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,516,739\n",
            "Trainable params: 4,514,819\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-53f548a85824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"losses.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-53f548a85824>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(ndf, ngf, z_dim, lr_d, lr_g, epochs, batch_size, epoch_per_checkpoint, n_checkpoint_images)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# update Generator weights, do not update Discriminator weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_of_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mlosses_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2091\u001b[0m                                                     class_weight)\n\u001b[1;32m   2092\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2093\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2095\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x1152 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}